\documentclass[landscape,footrule]{foils}
\usepackage[lecture-serie]{foiltex-extra}
\usepackage{crysymb}
\usepackage{graphics}
\usepackage[pdftex]{graphicx} 




\newcommand{\lecture}{Graphical models}
\newcommand{\lserie}{LTAT.02.004 Machine Learning II}
\newcommand{\ldate}{March 13, 2019}
\newcommand{\lauthor}{Sven Laur}
\newcommand{\linst}{University of Tartu}
\graphicspath{{./illustrations/}}


\newcommand{\leqm}{\ \leq_m}


\newcommand{\bigvskip}{\vskip 2em}
\newcommand{\lastline}{\vspace*{-2ex}}
\newcommand{\spreadappart}{\vspace*{\fill}}

\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\conf}{conf}
\DeclareMathOperator{\precision}{precision}
\DeclareMathOperator{\recall}{recall}


\begin{document}
\titlefoil


\foilhead[-1cm]{Discrete random variables}

\enlargethispage{0.9cm}
\begin{triangles}
\item A \emph{random variable} $X$ with possible \emph{outcomes} $x\in\supp(X)$
\item Compact notation for probabilities \vspace*{-1ex}
\begin{align*}
\pr{x_1}&:=\pr{\xi\gets X_1: \xi=x_1}\\
\pr{x_1\wedge x_2}&:=\pr{\xi_1\gets X_1,\xi_2\gets X_2 : \xi_1=x_1\wedge \xi_2=x_2}
\end{align*}\vspace*{-4ex}
\item Bayes formula \vspace*{-2ex}
\begin{align*}
\pr{a|b}=\frac{\pr{a\wedge b}}{\pr{b}}=\frac{\pr{b|a}\pr{a}}{\pr{b}}
\end{align*}\vspace*{-4ex}
\item Independence of random variables $X_1\ldots X_m\perp Y_1,\ldots Y_n$: \vspace*{-1ex}
\begin{align*}
 \pr{x_1\wedge\ldots\wedge x_m\wedge y_1\wedge\ldots\wedge y_n}=\pr{x_1\wedge\ldots\wedge x_m}\cdot\pr{y_1\wedge\ldots\wedge y_n}
\end{align*}\vspace*{-4ex}
\item Marginalisation over variables $Y_1,\ldots, Y_n$: \vspace*{-1ex}
\begin{align*}
 \pr{x_1\wedge\ldots\wedge x_m}=\sum_{y_1,\ldots,y_n}\pr{x_1\wedge\ldots\wedge x_m \wedge y_1\wedge\ldots\wedge y_n}
\end{align*} 
\end{triangles}

\middlefoil{Common models}

\foilhead[-1cm]{Markov chain}
\illustration[scale=1.5]{markov-chain}

\textbf{Definition.}
Let $X_1, X_2,\ldots$ be correlated random variables such that the probability of the observation $x_{i+1}$ depends only on the observation $x_{i}$.
Then the entire process is known as Markov chain.

\vspace*{1cm}

\textbf{Parametrisation.}
Markov chain is determined by specifying 

\begin{triangles}
\item state spaces $\SSS_1\ldots,\SSS_n$
\item initial probabilities $\pr{x_1}$
\item state transition probabilities $\pr{x_{i+1}|x_{i}}$
\end{triangles}

\foilhead[-1cm]{What questions can we ask?}

\textbf{Sampling:} What are typical outcomes of the chain?\\
$\triangleright$ Synthesis of time-series, textures, sounds, games movements. 
\vspace*{1.5ex} 

\textbf{Stationary distribution:} What happens if we run the chain infinitely long?\\
$\triangleright$ Getting samples from an unnormalised posterior, optimisation tasks. 
\vspace*{1.5ex}  

\textbf{Likelihood estimation:} What is a probability of an observation $x_1,\ldots,x_n$?\\
$\triangleright$ Reasoning about probabilities and clustering sequences.
\vspace*{1.5ex} 

\textbf{Decoding:} What is the most probable outcome $x_1,\ldots,x_n$?\\ 
$\triangleright$ Imputing missing values. Rudimentary logical reasoning. 
\vspace*{1.5ex} 

\textbf{Parameter estimation:} What is are the model parameters?\\
$\triangleright$ Machine learning -- finding parameters based on observations.

\foilhead[-1cm]{Parameter inference for homogenous case}

\illustration[scale=1.5]{homogenous-markov-chain}

For a sequence of observations $\vec{x}=(x_1,\ldots, x_{n})$ the log-likelihood is
\begin{align*}
\ell[\vec{x}]&=\log \underbrace{\pr{x_1}}_{\beta[x_1]} + \sum_{i=1}^{n-1} \log \underbrace{\pr{x_{i+1}|x_{i}}}_{\alpha[x_i, x_{i+1}]} \\
&=\log \beta[x_1,\ldots,x_m] + \sum_{u_1,u_2} k(u_1,u_2)\log\alpha[u_1,u_2]
\end{align*}
where $k(u_1,u_2)$ is the count of bigrams $u_1, u_2$ in the sequence $\vec{x}$.

\foilhead[-1cm]{Posterior decomposition}

\enlargethispage{0.5cm}
As a result the log-likelihood of unnormalised posterior decomposes into the sum of independent terms
\begin{align*}
\log p[\vec{\alpha},\vec{\beta}|\vec{x}]
=&\sum_{u_1} k(u_1)\log \beta[u_1] + \log p(\vec{\beta})\\
+&\sum_{u_1,u_2} k(u_1,u_2)\log\alpha[u_1,u_2]+\sum_{u_1}\log p(\vec{\alpha}[u_1,\cdot]) 
\end{align*}
where 
\begin{triangles}
\item $k(u_1)$ is the count $u_1$ at the beginning of the observed sequences
\item $k(u_1,u_2)$ is the count of bigrams $u_1, u_2$ in the observed sequences.
\item $p(\vec{\beta})$ is the prior for an entire vector of initial probabilities
\item $p(\vec{\alpha}[u_1,\cdot])$ is the prior for the transition probabilities from $u_1$ 
\end{triangles}

\foilhead[-1cm]{Reduction to the dice throwing experiment}
\enlargethispage{4cm}
Posterior decomposition leads to many independent optimisation tasks\vspace*{-2ex}
\begin{align*}
&\sum_{u_1} k(u_1)\log \beta[u_1] + \log p(\vec{\beta})\to\max\\
&\sum_{u_2} k(u_1,u_2)\log\alpha[u_1,u_2]+\log p(\vec{\alpha}[u_1,\cdot])\to\max 
\end{align*}\vspace*{-4ex} \ \\
where each of these is equivalent to optimisation of dice throwing posterior. 
Thus Maximum Aposteriori estimates for parameters are \vspace*{-2ex}
\begin{align*}
\beta[u_1]&=\frac{k(u_1)+c}{k(*)+mc} &
\alpha[u_1,u_2]&=\frac{k(u_1,u_2)+c}{k(u_1,*)+mc}
\end{align*}\vspace*{-4ex}\ \\
where
\begin{triangles}
\item $*$ is a wildcard symbol in the count queries
\item $m$ is the number of  states and $c$ is a constant for Laplacian smoothing. 
\end{triangles}
 

\foilhead[-1cm]{Hidden Markov Model}
\illustration[scale=1.5]{hidden-markov-model}

\textbf{Definition.}
Let $X_1,X_2,\ldots$ be hidden states that form a Markov chain and let $Y_1,Y_2,\ldots$ be observations that the probability of $y_i$ depends only on the state $x_i$. Then the entire process is known as Hidden Markov Model.\vspace*{1ex}

\textbf{Common tasks}
\begin{triangles}
\item parameter estimation 
\item filtering, smoothing, prediction
\end{triangles}


\foilhead[-1cm]{Applications}

\textbf{Modelling and prediction}
\begin{triangles}
\item stock prices
\item linear control algorithms 
\end{triangles}\vspace*{2ex}

\textbf{Sequence annotation}
\begin{triangles}
\item fraud detection
\item change detection 
\item functional motifs of DNA sequences
\end{triangles}\vspace*{2ex}


\textbf{Decoding}
\begin{triangles}
\item speech recognition
\item communication over a nosy channels 
\item object tracking and data fusion
\end{triangles}


\foilhead[-1cm]{Random Markov Fields}

\illustration[scale=1.2]{markov-random-field}

\textbf{Definition.}
Markov random field is specified by undirected graph connecting random variables $X_1,X_2,\ldots$ such that for any node $X_i$ 
\begin{align*}
\pr{x_i|(x_j)_{j\neq i}}=\pr{x_i| (x_j)_{j\in\NNN(X_i)} }
\end{align*}
where the set of neighbours $\NNN(X_i)$ is also known as \emph{Markov blanket} for $X_i$. 


\foilhead[-1cm]{Hammersley-Clifford theorem}

The probability of an observation $\vec{x}=(x_1,x_2,\ldots)$ generated by a Markov random field can be expressed in the form 
\begin{align*}
\pr{\vec{x}}=\frac{1}{Z(\omega)}\cdot\exp{-\sum_{c\in\textsf{MaxClique}}\Psi_c(\vec{x}_c,\omega)} 
\end{align*}  
where
\begin{triangles}
\item $Z(\omega)$ is a normalising constant
\item $\textsf{MaxClique}$ is the set of maximal cliques in the Markov random field
\item $\Psi_c$ is defined on the variables in the clique $c$ 
\end{triangles}


\foilhead[-1cm]{Conditional Random Fields}

\illustration[scale=1.5]{conditional-random-field}

\textbf{Definition.}
Let $X_1,X_2,\ldots$ and $Y_1,Y_2,\ldots$ be random variables. The entire process is conditional random field if random variables $Y_1,Y_2,\ldots$ conditioned for any sequence of observations $x_1,x_2,\ldots$ form a Markov random field
\begin{align*}
\pr{y_i|(x_k)_{k=1}^\infty, (y_j)_{j\neq i}}=\pr{y_i|(x_k)_{k=1}^\infty, (y_j)_{j\in\NNN(Y_i)} }
\end{align*}
where the set of neighbours $\NNN(Y_i)$ is a \emph{conditional Markov blanket} for $Y_i$. 

\foilhead[-1cm]{Applications}

\textbf{Standard setting}
\begin{triangles}
\item The input $\vec{x}$ is used to predict labels $y_1,y_2,\ldots$.
\item A correct label sequence must satisfy possibly unknown restrictions.
\item These restrictions are captured by conditional random random field.
\end{triangles}\vspace*{1cm}


\textbf{Instantiation}
\begin{triangles}
\item Hammersley-Clifford theorem prescribes the format of $\pr{\vec{y}|\vec{x}}$
\item Clique features $\Psi_c$ can depend on $(y_i)_{i\in c}$, $(x_i)_{i=1}^\infty$ 
\item Features can be defined as linear combination of vertex and edge features.
\item A vertex feature looks only variable $y_i$ associated with the vertex.
\item An edge feature looks only variables $y_i, y_j$ associated with the edge.
\end{triangles}



\middlefoil{Belief propagation}

\foilhead[-1cm]{Belief propagation in a chain}

\illustration[scale=1.5]{belief-propagation-in-chain-i}

\textbf{Evidence}
\begin{triangles}
\item We know the values $a$ and $e$ for nodes $A$ and $E$. 
\item We know a value distribution for nodes $A$ and $E$. 
\end{triangles}\vspace*{2ex}

\textbf{Representation}
\begin{triangles}
\item A prior vector $\pi_A$ will represent value distribution in $A$.
\item A likelihood vector $\lambda_E$ will represent value distribution in $E$.
\end{triangles}

\foilhead[-1cm]{Belief propagation in a chain}

\illustration[scale=1.5]{belief-propagation-in-chain-ii}

\textbf{Iterative propagation}
\begin{triangles}
\item Marginalisation gives an update rule $\lambda_D=M_{e|d}\lambda_E$.
\item Marginalisation gives an update rule $\pi_B\propto \pi_A M_{a|b}$.

\end{triangles}

\foilhead[-1cm]{Belief propagation in a chain}

\illustration[scale=1.5]{belief-propagation-in-chain-iii}

\textbf{Evidence pooling}
\begin{triangles}
\item Marginal conditional probability $p_c=\pr{c|\textsf{evidence}}\propto \pi_C\lambda_C$
\end{triangles}

\foilhead[-1cm]{Likelihood propagation in a tree}

\illustration[scale=1.2]{likelihood-propagation-in-tree-i}


\textbf{Iterative propagation}
\begin{triangles}
\item Independence gives an pooling rule $\lambda_D=\lambda_1\lambda_2$
\item Marginalisation gives rules $\lambda_1= M_{e|d}\lambda_E$ and $\lambda_2= M_{e|f}\lambda_F$.
\end{triangles}


\foilhead[-1cm]{Prior propagation in a tree}
\illustration[scale=1.2]{prior-propagation-in-tree-i}

\textbf{Evidence pooling}
\begin{triangles}
\item Marginal conditional probability $p_A=\propto \pi_A\lambda_A$
\end{triangles}


\foilhead[-1cm]{Prior propagation in a tree}
\illustration[scale=1.2]{prior-propagation-in-tree-ii}

\textbf{Iterative propagation}
\begin{triangles}
\item Prior component can be updates $\pi_D\propto\frac{p_A}{M_{d|a}\lambda_D}$
\end{triangles}

\end{document}

\foilhead[-1cm]{Reduction to the previous building blocks}

The problem simplifies when we know the vector of hidden states $\vec{x}$: 
\begin{triangles}
\item Inference of emission probabilities $\delta[\ldots]$ reduce to dice throwing.
\item Inference of the chain parameters $\alpha[\ldots]$ and $\beta[\ldots]$ is also possible.
\end{triangles}
\vspace*{2ex}

We can use Viterbi algorithm for finding the most probable hidden state $\vec{x}$ is easy if all parameters are known.
\vspace*{2ex}


\textbf{Naive inference algorithm:} Fix random parameters and repeat steps:
\begin{triangles}
\item Given parameters $\alpha, \beta, \delta$  learn the most probable hidden state $\vec{x}$.
\item Given the most probable hidden state $\vec{x}$ learn model parameters $\alpha, \beta, \delta$.
\end{triangles}
\vspace*{2ex}
This algorithm overfits as all hidden states can have similar probability. 


\foilhead[-0cm]{Model behind naive Bayes classifier}

%\illustration[scale=0.8]{naive-bayes-scheme}

Underlying class value determines observed attributes 
\begin{triangles}
\item Each attribute $X_i$ is binary 
\item All variables are independent if class is fixed
\item Sometimes we just ignore dependancies for easier modelling
\end{triangles}

\foilhead[-0cm]{Likelihood of the data}

Let us assume that we know the probabilities
\begin{align*}
p_i&=\pr{X_i=1|Class=0}\\
q_i&=\pr{X_i=1|Class=1}
\end{align*}
Then using the independence assumption we get
\begin{align*}
\pr{X_1=a_1,\ldots,X_n=a_n|Class=0}&=\prod_{i=1}^np_i^{a_i}(1-p_i)^{1-a_i}\\
\pr{X_1=a_1,\ldots,X_n=a_n|Class=1}&=\prod_{i=1}^nq_i^{a_i}(1-q_i)^{1-a_i}
\end{align*}

\foilhead[-1cm]{Prior and posterior for the class labels}

\enlargethispage{1.5cm}
Now it is straightforward to derive
\begin{align*}
\pr{Class=0|\vec{X}=\vec{a}}&= \frac{\prod\limits_{i=1}^np_i^{a_i}(1-p_i)^{1-a_i}\cdot\pr{Class=0}}{\pr{\vec{X}=\vec{a}}}\\
\pr{Class=1|\vec{X}=\vec{a}}&= \frac{\prod\limits_{i=1}^nq_i^{a_i}(1-q_i)^{1-a_i}\cdot\pr{Class=1}}{\pr{\vec{X}=\vec{a}}}
\end{align*}
which gives an \emph{odd ratio} 
\begin{align*}
\frac{\pr{Class=0|\vec{X}=\vec{a}}}{\pr{Class=1|\vec{X}=\vec{a}}}&=\frac{\pr{Class=0}}{\pr{Class=1}}\cdot\frac{\prod\limits_{i=1}^np_i^{a_i}(1-p_i)^{1-a_i}}{\prod\limits_{i=1}^nq_i^{a_i}(1-q_i)^{1-a_i}}
\end{align*} 
 
\foilhead[-1cm]{The resulting classifier is a linear classifer}
 
By taking logarithm form the odd ratio we get
\begin{align*}
\log\left(\frac{\pr{Class=0|\vec{X}=\vec{a}}}{\pr{Class=1|\vec{X}=\vec{a}}}\right)&= w_0+\sum_{i=1}^n w_ia_i
\end{align*} 
where 
\begin{align*}
w_0&=\log\left(\frac{\pr{Class=0}}{\pr{Class=1}}\right)+\sum_{i=1}^n\log\left(\frac{1-p_i}{1-q_i}\right)\\
w_i&=\log \left(\frac{p_i}{1-p_i}\cdot\frac{1-q_i}{q_i}\right) 
\end{align*}

\foilhead[-1cm]{How to train the classifier?}
A frequentistic approach is to fix probabilities from the training sample
\begin{align*}
p_i&=\frac{\#\set{\text{data points form class 0 with $X_i=1$}}}{\#\set{\text{data points form class 0}}}\\
q_i&=\frac{\#\set{\text{data points form class 1 with $X_i=1$}}}{\#\set{\text{data points form class 1}}}
\end{align*}
However if some value does not occur for $X_i$ in the training sample we get overly confident results. Thus, Bayesian mean estimate is better alternative  
\begin{align*}
p_i&=\frac{\#\set{\text{data points form class 0 with $X_i=1$}}+1}{\#\set{\text{data points form class 0}}+2}\\
q_i&=\frac{\#\set{\text{data points form class 1 with $X_i=1$}}+1}{\#\set{\text{data points form class 1}}+2}
\end{align*}

\end{document}

\foilhead[-1cm]{Going beyond naive Bayesian models}
\illustration[scale=0.8]{bayesian-network}

Complex causal models are often defined through Bayesian networks
\begin{triangles}
\item A complex processes is first split into sub-events
\item Direct causal dependencies between sub-events are detected
\item Causation mechanisms are characterised with probability tables
\end{triangles} 
  
\foilhead[-1cm]{Strength and weaknesses of Bayesian networks}

\textbf{Strengths}
\begin{triangles}
\item Bayesian networks are easy to interpret
\item Bayesian networks are good for formalising fuzzy background knowledge
\item Estimation of individual probability tables is tractable
\item There are tools for doing inference with Bayesian networks  
\end{triangles}
\vspace*{1cm}

\textbf{Weaknesses}
\begin{triangles}
\item You must know the causal structure of sub-events  
\item Identification of causal structure form data alone is very difficult
\item It is notoriously difficult to model non-trivial causal dependencies
\item Standard inference procedures often do not have closed solutions 
\end{triangles}

\end{document}
