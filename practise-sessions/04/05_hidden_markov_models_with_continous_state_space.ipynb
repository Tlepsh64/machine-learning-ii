{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models with continous state space\n",
    "\n",
    "* Many physical system can be modelled with [Markov chains with continous state space](04_markov_chains_with_continous_state_space.ipynb).\n",
    "* Unfortunately, the indernal state of the system is not directly observable.\n",
    "* By adding a model for the measurement procedure we get a Hidden Markov model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import sklearn\n",
    "import string\n",
    "\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "from typing import List,Tuple\n",
    "\n",
    "from pandas import Categorical\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from tqdm import tnrange#, tqdm_notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from plotnine import *\n",
    "\n",
    "# Local imports\n",
    "from common import *\n",
    "from convenience import *\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import multivariate_normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Hidden Markov models with a real-valued state\n",
    "\n",
    "Let us consider a simple Hidden Markov model\n",
    "\n",
    "\\begin{align*}\n",
    "x_{i+1}&=ax_i+ w_i\\\\\n",
    "y_i&=cx_i+v_i \n",
    "\\end{align*}\n",
    "\n",
    "where $y_i$ is the observable quantity and $x_i$ is the hidden system state.\n",
    "* Nonlinearity $w_i$ is modelled by a normal distribution $\\mathcal{N}(0, \\sigma_i)$. \n",
    "* Measurement noise $v_i$ is modelled by a normal distribution $\\mathcal{N}(0, \\tau_i)$.\n",
    "* Quantities $x_0, w_1, \\ldots, w_n, v_1, \\ldots, v_n$ are assumed to be independent.\n",
    "* The initial state $x_0$ is distributed according to the normal distribution $\\mathcal{N}(\\mu_0, \\sigma_0)$. \n",
    "\n",
    "### Standard questions \n",
    "\n",
    "Similarly to Hidden Markov moddels with discrete statespace we can as following questions:\n",
    "* What is the distribution of $x_i$ given observations $y_1,\\ldots, y_{i-1}$?\n",
    "\n",
    "* What is the distribution of $x_i$ given observations $y_{i+1},\\ldots, y_{n}$?\n",
    "\n",
    "* What is the distribution of $x_i$ given observations $y_{1},\\ldots, y_{n}$?\n",
    "\n",
    "\n",
    "\n",
    "To answer the first question we need to compute prior \n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_{X_i}(x_i)=p[x_i|y_1\\ldots, y_{i-1}]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "To answer the second question we need to compute likelihood\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_{X_i}(x_i)=p[y_{i+1},\\ldots, y_n|x_i]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "To answer the third question we need to compute the marginal posterior\n",
    "\n",
    "\\begin{align*}\n",
    "p_{X_i}(x_i)=p[x_i|y_1,\\ldots, y_n]\\propto \\pi_{X_i}(x_i)\\cdot p[y_i|x_i]\\cdot \\lambda_{X_i}(x_i) \\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Conditional normal distribution as information fusion gate\n",
    "\n",
    "* problem\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Prior propagation\n",
    "\n",
    "We use same tricks to simplify the derivation of prior propagation rules.\n",
    "\n",
    "* Evolution of states conditioned on observations $y_1,\\ldots, y_{i-1}$\n",
    "\n",
    "\\begin{align*}\n",
    "x_i=\\mu_i+\\varepsilon_i,\\qquad \\varepsilon_i\\sim\\mathcal{N}(0, \\rho_i)\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "* Evolution of observations conditioned on $x_i$\n",
    "\n",
    "\\begin{align*}\n",
    "y_i=cx_i+v_i= c(\\mu_i+\\varepsilon_i)+ v_i=c\\mu_i + c\\varepsilon_i+ v_i\\enspace.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base\n",
    "\n",
    "By definition $x_1=a x_0+w_0$ for $w_0\\sim\\mathcal{N}(0, \\sigma_0)$ and thus $\\rho_1=\\sigma_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General induction step\n",
    "\n",
    "Assume that $p[x_{i-1}|y_{1},\\ldots, y_{i-1}]$ is a normal distribution $\\mathcal{N}(\\mu_{i-1},\\rho_{i-1})$ and we can express \n",
    "\n",
    "\\begin{align*}\n",
    "x_{i-1}=\\mu_{i-1}+\\varepsilon_{i-1}, \\qquad \\varepsilon_{i-1}\\sim\\mathcal{N}(0,\\rho_{i-1})\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Now let us simplify the probability \n",
    "\n",
    "\\begin{align*}\n",
    "p[x_i, x_{i-1}|y_1,\\ldots, y_{i-1}]\n",
    "&= p[x_i| x_{i-1},y_1,\\ldots, y_{i-1}]\\cdot p[x_{i-1}|y_1,\\ldots, y_{i-1}]\\\\\n",
    "&= p[x_i| x_{i-1}]\\cdot p[x_{i-1}|y_1,\\ldots, y_{i-1}]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note that the right term can be further expanded\n",
    "\n",
    "\\begin{align*}\n",
    "p[x_{i-1}|y_1,\\ldots, y_{i-1}] \n",
    "&=\\frac{p[y_{i-1}|x_{i-1},y_1,\\ldots, y_{i-2}]\\cdot p[x_{i-1}|y_1,\\ldots, y_{i-2}]}{p[y_{i-1}|y_1,\\ldots, y_{i-1}]}\\\\\n",
    "& \\propto p[y_{i-1}|x_{i-1}]\\cdot p[x_{i-1}|y_1,\\ldots, y_{i-2}]\\enspace.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we can simplify\n",
    "\n",
    "\\begin{align*}\n",
    "p[x_{i-1}|y_1,\\ldots, y_{i-1}] \n",
    "&\\propto\n",
    "\\exp\\Biggl(-\\frac{(y_{i-1}-cx_{i-1})^2}{2\\tau_{i-1}^2}\\Biggr)\\cdot\n",
    "\\exp\\Biggl(-\\frac{(x_{i-1}-\\mu_{i})^2}{2\\rho_{i-1}^2}\\Biggr)\\\\\n",
    "&\\propto\\exp\\Biggl(-\\frac{\\rho_{i-1}^2(y_{i-1}-cx_{i-1})^2 + \\tau_{i-1}^2(x_{i-1}-\\mu_{i})^2}{2\\tau_{i-1}^2\\rho_{i-1}^2}\\Biggr)\\\\\n",
    "&\\propto\\exp\\Biggl(-\\frac{(\\rho_{i-1}^2c^2+ \\tau_{i-1}^2)x_{i-1}^2-2(\\rho_{i-1}^2y_{i-1}c -2\\tau_{i-1}^2\\mu_{i})x_{i-1}}{2\\tau_{i-1}^2\\rho_{i-1}^2}\\Biggr)\\\\\n",
    "&\\propto\\exp\\Biggr(-\\frac{(\\rho_{i-1}^2c^2+ \\tau_{i-1}^2)\\bigl(x_{i-1}-\\frac{\\rho_{i-1}^2y_{i-1}c -2\\tau_{i-1}^2\\mu_{i}}{\\rho_{i-1}^2c^2+ \\tau_{i-1}^2}\\bigr)^2}{2\\tau_{i-1}^2\\rho_{i-1}^2}\\Biggr)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "and thus $x_{i-1}$ conditioned on the observations $y_1,\\ldots, y_{i-1}$ has a normal distribution $\\mathcal{N}(\\mu_{i-1}^*, \\sigma_{i-1}^*)$ with the parameters\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_{i-1}^*&=\\frac{\\rho_{i-1}^2y_{i-1}c -2\\tau_{i-1}^2\\mu_{i}}{\\rho_{i-1}^2c^2+ \\tau_{i-1}^2}\\\\\n",
    "\\sigma_{i-1}^{*2}&=\\frac{\\tau_{i-1}^2\\rho_{i-1}^2}{\\rho_{i-1}^2c^2+ \\tau_{i-1}^2}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the joint probability can be expressed\n",
    "\n",
    "\\begin{align*}\n",
    "p[x_i, x_{i-1}|y_1,\\ldots, y_{i-1}]&\\propto\n",
    "\\exp\\Biggl(-\\frac{(x_i-ax_{i-1})^2}{2\\sigma_{i-1}^2}\\Biggr)\\cdot\n",
    "\\exp\\Biggl(-\\frac{(x_{i-1}-\\mu_{i-1}^*)^2}{2\\sigma_{i-1}^{*2}}\\Biggr)\\\\\n",
    "&\\propto\n",
    "\\exp\\Biggl(-\\frac{\\sigma_{i-1}^{*2}(x_i-ax_{i-1})^2+\\sigma_{i-1}^2(x_{i-1}-\\mu_{i-1}^*)^2}{2\\sigma_{i-1}^2\\sigma_{i-1}^{*2}}\\Biggr)\\\\\n",
    "&\\propto\n",
    "\\exp\\Biggl(-\\frac{\\sigma_{i-1}^{*2}x_i^2+ (\\sigma_{i-1}^{*2}a^2++\\sigma_{i-1}^2)x_{i-1}^2 -2\\sigma_{i-1}^{*2}ax_ix_{i-1}  -2\\sigma_{i-1}^2x_{i-1}\\mu_{i-1}^*}{2\\sigma_{i-1}^2\\sigma_{i-1}^{*2}}\\Biggr)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "which is a dendity function of a two-dimensional normal distrinution with parameters\n",
    "\n",
    "\\begin{align*}\n",
    "??\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the fact that marginal distribution of a normal distrinution is also a normal distribution and express the parameters of the resulting normal distribution\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_i&= ??\\\\\n",
    "\\sigma_i^2&= ??\\\\\n",
    "\\end{align*}\n",
    "\n",
    "This completes the recursion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Likelihood propagation\n",
    "\n",
    "The derivation of likelihood propagation formulae is exactly analogous. Also note that we can derive these formulae by revering the chain and using already derived formulae for belief propagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Marginal posterior\n",
    "\n",
    "The derivation of marginal posterior is also analogous. Again note that engineers use maximal aposteriori estimate and thus the estimate of mean is often needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
