{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chains with continous state space\n",
    "\n",
    "* Dynamics of physical systems is particularly important in robotics.\n",
    "* Dynamics of most physical systems are determined by differential equations.\n",
    "* Differential equations are used to describe other dynamically evolving systems.  \n",
    "* These equations are usually solved with numerical methods that approximate differentials.\n",
    "* Usage of these approximations leads to linear update rules for the next state \n",
    "\n",
    "  \\begin{align*}\n",
    "  \\boldsymbol{x}_{i+1}=A\\boldsymbol{x}_i+\\boldsymbol{w}_i \n",
    "  \\end{align*}\n",
    "\n",
    "  where $\\boldsymbol{x}_{i+1}$ is the actual outcome, $A\\boldsymbol{x}_i$ is an approximation and  $\\boldsymbol{w}_i$ is unknown error term.\n",
    "* A numerical approximation method is good if the average error $\\mathbf{E}(\\boldsymbol{w}_i)$ is near zero.\n",
    "* Otherwise the numerical approximation method has a systematic bias that should be removed.\n",
    "* Such a system can viewed as Markov chain if we additionally assume that errors $\\boldsymbol{w}_i$  are independent. \n",
    "* Note that the state space for $\\boldsymbol{x}_i$ is a continous vector space $\\mathbb{R}^\\ell$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import sklearn\n",
    "import string\n",
    "\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "from typing import List,Tuple\n",
    "\n",
    "from pandas import Categorical\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from tqdm import tnrange#, tqdm_notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from plotnine import *\n",
    "\n",
    "# Local imports\n",
    "from common import *\n",
    "from convenience import *\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Markov chains with a real-valued state\n",
    "\n",
    "\n",
    "Let us consider a swinging pendulum and let $x_i$ be its height at the $i$-th iteration:\n",
    "* We start by realeasing the pendulum at the height $x_0$.\n",
    "* Then it drops and raises again $x_1$ is its highest position after which its starts to drop again.\n",
    "\n",
    "### System dynamics\n",
    "\n",
    "Due to the friction the pendulum looses energy. We model it through the following equation \n",
    "\n",
    "\\begin{align*}\n",
    "x_{i+1}=a x_{i} + w_i \n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "* the coeffient $a$ detrmines how fast the system looses energy \n",
    "* the error term $w_i$ captures the impact of other forces like wind. \n",
    "\n",
    "We model the effect other forces as follows:\n",
    "* We assume that all error terms $w_i$ are independent.   \n",
    "* We assume the each error term $w_i$ is distributed according $\\mathcal{N}(0,\\sigma_i)$.\n",
    "* We assume that the initial position $x_0$ is fixed.\n",
    "\n",
    "Then description of the system is complete. It is Markov chain.\n",
    "\n",
    "### Standard questions\n",
    "\n",
    "Similarly to Markov chains with discrete statespace we can as following questions:\n",
    "* What is the position of the pendulum at the $i$-th iteration if we know only the initial state $x_0$?\n",
    "\n",
    "* What is the position of the pendulum at the $i$-th iteration if we know only the final state $x_n$?\n",
    "\n",
    "* What is the position of the pendulum at the $i$-th iteration if we know  both states $x_0$ and $x_n$?\n",
    "\n",
    "\n",
    "To answer the first question we need to compute prior \n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_{X_i}(x_i)=p[x_i|x_0]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "To answer the second question we need to compute likelihood\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_{X_i}(x_i)=p[x_n|x_i]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "To answer the third question we need to compute the marginal posterior\n",
    "\n",
    "\\begin{align*}\n",
    "p_{X_i}(x_i)=p[x_i|x_0,x_n]\\propto \\pi_{X_i}(x_i)\\cdot \\lambda_{X_i}(x_i) \\enspace.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Prior propagation\n",
    "\n",
    "There are two ways to derive expressions for the prior function:\n",
    "\n",
    "* direct application of [prior propagation rules](../03/belief_propagation_in_a_tree.ipynb) \n",
    "* use of linear expressions that separate the effect of state from random error components.  \n",
    "\n",
    "Both appraches lead to the same end result, but the first apprach is very technical and leads to many integrals.\n",
    "Therefore, we pursue the second approach and iteratively express\n",
    "\n",
    "\\begin{align*}\n",
    "x_{i}=\\mu_{i}+\\varepsilon_{i},\\qquad \\varepsilon_i\\sim\\mathcal{N}(0,\\rho_{i})\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "The latter allows us to escape techincalities and use only the fact that linear combination of independent normal distributions is also  normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closeness under linear combinations\n",
    "\n",
    "Linear combination $v=\\alpha_1 u_1+\\alpha_2 u_2+\\cdots+\\alpha_n u_n$ of independent univariate normal distributions\n",
    "\n",
    "\\begin{align*}\n",
    "u_1&\\sim\\mathcal{N}(\\mu_1,\\sigma_1)\\\\\n",
    "u_2&\\sim\\mathcal{N}(\\mu_2,\\sigma_2)\\\\\n",
    "\\cdots&\\sim\\cdots\\\\\n",
    "u_n&\\sim\\mathcal{N}(\\mu_n,\\sigma_n)\n",
    "\\end{align*}\n",
    "\n",
    "is also a normal distribution $\\mathcal{N}(\\mu, \\sigma)$ with parameters\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu&=\\alpha_1\\mu_1+\\alpha_2\\mu_2+\\cdots+\\alpha_n\\mu_n\\\\\n",
    "\\sigma^2&=\\alpha_1^2\\sigma_1^2+\\alpha_2^2\\sigma_2^2+\\cdots+\\alpha_n^2\\sigma_n^2\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "**Justification for paramater expressions.**\n",
    "If $v$ has normal distribution we can find its parameters through moment matching. \n",
    "Linearity of mathematical expectaion gives\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu\n",
    "&=\\mathbf{E}(\\alpha_1 u_1+\\alpha_2 u_2+\\cdots+\\alpha_n u_n)\\\\\n",
    "&=\\mathbf{E}(\\alpha_1 u_1)+ \\mathbf{E}(\\alpha_2 u_2)+\\cdots+\\mathbf{E}(\\alpha_n u_n)\\\\\n",
    "&=\\alpha_1\\mu_1+\\alpha_2\\mu_2+\\cdots+\\alpha_n\\mu_n\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Independence assumption together with the properties of variance gives\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^2\n",
    "&=\\mathbf{D}(\\alpha_1 u_1+\\alpha_2 u_2+\\cdots+\\alpha_n u_n)\\\\\n",
    "&=\\mathbf{D}(\\alpha_1 u_1)+ \\mathbf{D}(\\alpha_2 u_2)+\\cdots+\\mathbf{D}(\\alpha_n u_n)\\\\\n",
    "&=\\alpha_1^2\\sigma_2+\\alpha_2^2\\sigma_2^2+\\cdots+\\alpha_n^2\\sigma_n^2\\enspace.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base\n",
    "\n",
    "By definition of the normal distribution we can write $x_1=a x_0 + w_0$ for $w_0\\sim\\mathcal{N}(0, \\sigma_0)$ and thus $\\rho_1=\\sigma_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General induction step\n",
    "\n",
    "Assume that $p[x_{i-1}|x_{0}]$ is a normal distribution $\\mathcal{N}(\\mu_{i-1},\\rho_{i-1})$ and we can express \n",
    "\n",
    "\\begin{align*}\n",
    "x_{i-1}=\\mu_{i-1}+\\varepsilon_{i-1}, \\qquad \\varepsilon_{i-1}\\sim\\mathcal{N}(0,\\rho_{i-1})\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "As $x_{i+1}=ax_i+w_{i}$ we get\n",
    "\n",
    "\\begin{align*}\n",
    "x_{i}=a(\\mu_{i-1}+\\varepsilon_{i-1})+w_{i}=a\\mu_{i-1}+a\\varepsilon_{i-1}+w_i\n",
    "\\end{align*}\n",
    "\n",
    "and thus we can define\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_{i}&=a \\mu_{i-1}\\\\\n",
    "\\varepsilon_{i} &= a\\varepsilon_{i-1} +\\varepsilon_{i}\\enspace\n",
    "\\end{align*}\n",
    "\n",
    "to preserve the induction hypotesis. Again, $\\varepsilon_i$ is linear compination of independent normal distributions and thus must have a normal distribution $\\mathcal{N}(0, \\rho_i)$. \n",
    "Moment matching yields \n",
    "\n",
    "\\begin{align*}\n",
    "\\rho_i^2=a^2\\rho_{i-1}+\\sigma_i^2\\enspace.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical example \n",
    "\n",
    "Let us consider equation $x_{i+1}=0.9\\cdot x_{i}+w_i$ for $w_i\\sim\\mathcal{N}(0, 1)$ and $x_0\\sim\\mathcal{N}(10, 1)$. Let us sample 1000 parallel runs and match theoretical prior distributions with simulated prior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Likelihood propagation\n",
    "\n",
    "There are two ways to derive expressions for the likelihood function:\n",
    "\n",
    "* direct application of likelihood propagation rules \n",
    "* use of linear expression that separates the effect of state from random error components.  \n",
    "\n",
    "Both appraches lead to the same end result, but the first approach is very technical and leads to many integrals.\n",
    "Therefore, we pursue the second approach and iteratively express\n",
    "\n",
    "\\begin{align*}\n",
    "x_{n}=\\alpha_{i}x_{i}+\\varepsilon_{i},\\qquad \\varepsilon_i\\sim\\mathcal{N}(0,\\delta_{i})\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Note that the mean value is expressed in terms of $x_i$ to make the induction step tractable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base\n",
    "\n",
    "By definition $x_{n}=ax_{n-1}+ w_{n-1}$ for $w_{n-1}\\sim\\mathcal{N}(0,\\sigma_{n-1})$ and thus trivially\n",
    "\n",
    "\\begin{align*}\n",
    "x_{n}=\\alpha_{n-1}x_{n-1}+\\varepsilon_{n-1},\\qquad \\varepsilon_{n-1}\\sim\\mathcal{N}(0, \\delta_{n-1})\n",
    "\\end{align*}\n",
    "\n",
    "for $\\alpha_{n-1}=a$ and $\\delta_{n-1}=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General induction step\n",
    "\n",
    "Assume that $p[x_n|x_{i+1}]$ is a normal distribution $\\mathcal{N}(\\alpha_{i+1}x_{i+1},\\delta_{i+1})$ and we can express \n",
    "\n",
    "\\begin{align*}\n",
    "x_n= \\alpha_{i+1}x_{i+1}+\\varepsilon_{i+1},\\qquad \\varepsilon_{i+1}\\sim\\mathcal{N}(0, \\delta_{i+1})\n",
    "\\end{align*}\n",
    "\n",
    "As $x_{i+1}=ax_i+w_i$ we can express\n",
    "\n",
    "\\begin{align*}\n",
    "x_n \n",
    "= \\alpha_{i+1}(ax_i+w_i)+\\varepsilon_{i+1}\n",
    "= \\alpha_{i+1} ax_i + \\alpha_{i+1}w_i +\\varepsilon_{i+1}\n",
    "\\end{align*}\n",
    "\n",
    "and thus we can define\n",
    "\n",
    "\\begin{align*}\n",
    "\\alpha_{i}&=\\alpha_{i+1}a\\\\\n",
    "\\varepsilon_{i} &= \\alpha_{i+1}w_i +\\varepsilon_{i+1}\\enspace\n",
    "\\end{align*}\n",
    "\n",
    "to preserve the induction hypotesis. As $\\varepsilon_i$ is linear combination of independent normal distributions, it must have a normal distribution $\\mathcal{N}(0, \\delta_i)$. \n",
    "Moment matching yields \n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_i^2=\\alpha_{i+1}^2\\sigma_i^2+ \\delta_{i+1}^2\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Therefore we get\n",
    "\n",
    "\\begin{align*}\n",
    "p[x_n|x_{n-1}]\n",
    "\\propto\\exp\\biggl(-\\frac{(x_n-\\alpha_{i}x_{i})^2}{2\\delta_{i}^2}\\biggr)\\enspace.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical example  continued\n",
    "\n",
    "Note that we can sample conditionditional distribution by reversing the chain. In general chain reversal is not always intractable but here the additive error model makes it straightforward\n",
    "\n",
    "\\begin{align*}\n",
    "x_i=\\frac{x_{i+1}-w_i}{a},\\qquad w_{i}\\sim\\mathcal{N}(0,1)\n",
    "\\end{align*}\n",
    "\n",
    "Thus we can indeen simulate what is the likelihood when $x_n$ is fixed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Marginal posterior\n",
    "\n",
    "Note that we can indeed use Bayes formula to express\n",
    "\n",
    "\\begin{align*}\n",
    "p[x_i| x_0, x_n]=\\frac{p[x_n|x_i, x_0]\\cdot p[x_i|x_0]}{p[x_n|x_0]}\n",
    "\\propto p[x_i|x_0]\\cdot p[x_n|x_i]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "As the prior and likelihood are normal ditributions, we get\n",
    "\n",
    "\\begin{align*}\n",
    "p[x_i| x_0, x_n]\n",
    "&\\propto\\exp\\Biggl(-\\frac{(x_i-\\mu_i)^2}{2\\rho_i^2}\\Biggr)\\cdot\\exp\\Biggl(-\\frac{(x_n-\\alpha_ix_i)^2}{2\\delta_i^2}\\Biggr)\\\\\n",
    "&\\propto\\exp\\Biggl(-\\frac{\\delta_i^2(x_i-\\mu_i)^2+ \\rho_i^2(x_n-\\alpha_ix_i)^2}{2\\rho_i^2\\delta_i^2}\\Biggr)\\\\\n",
    "&\\propto\\exp\\Biggl(-\\frac{\\delta_i^2x_i^2-2\\delta_i^2\\mu_ix_i+ \\rho_i^2\\alpha_i^2x_i^2-2\\rho_i^2x_n\\alpha_ix_i}{2\\rho_i^2\\delta_i^2}\\Biggr)\\\\\n",
    "&\\propto\\exp\\Biggl(-\\frac{(\\delta_i^2+\\rho_i^2\\alpha_i^2)x_i^2\n",
    "-2(\\delta_i^2\\mu_i+\\rho_i^2x_n\\alpha_i)x_i}{2\\rho_i^2\\delta_i^2}\\Biggr)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "And thus the marginal distribution $x_i|x_0,x_n$ follows indeed a normal distribution $\\mathcal{N}(\\mu, \\sigma)$ with parameters\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu&=\\frac{\\delta_i^2\\mu_i+\\rho_i^2x_n\\alpha_i}{\\delta_i^2+\\rho_i^2\\alpha_i^2}\\\\\n",
    "\\sigma^2&= \\frac{\\rho_i^2\\delta_i^2}{\\delta_i^2+\\rho_i^2\\alpha_i^2}\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Engineers are usually interested only on the maximal a posteriori estimate that is locaated in $\\mu$ and thus they completely ignore the other equation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical example continued\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.  Belief propagation formulae for higher dimensional state spaces\n",
    "\n",
    "* We can use the same linear equations to derive belief propagation formulae.\n",
    "* To make the notation consistent with standard treatments we use traditional notations for the update rule\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{x}_{i+1}= A\\boldsymbol{x}_i+\\boldsymbol{w}_i,\\qquad \\boldsymbol{w}_i\\sim\\mathcal{N}(\\boldsymbol{0}, Q_i) \\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "* <font color=\"red\">To be completed</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
