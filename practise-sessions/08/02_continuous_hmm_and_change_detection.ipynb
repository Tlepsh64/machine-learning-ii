{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous HMM and change detection\n",
    "\n",
    "Recall that in the HMM practice session we tried to detect wet and try seasons of Singapore\n",
    "\n",
    "* [03_change_detection_with_hidden_markov_models.ipynb](../03/03_change_detection_with_hidden_markov_models.ipynb)\n",
    "\n",
    "but at that time we did not know how to fit the model parameters. The aim of this exercise session is to complete the task again but now with parameter fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "from typing import Tuple\n",
    "\n",
    "from tqdm import tnrange#, tqdm_notebook\n",
    "from plotnine import *\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Local imports\n",
    "from common import *\n",
    "from convenience import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. E-step formulae for clustering and HMM\n",
    "\n",
    "For the clustering, we must assign a weight for each character image by rescaling the likelihood matrix. More precisely, let $(p_{ij})$ be the probability that the $i$th image belongs to the $j$th cluster then \n",
    "\\begin{align*}\n",
    " w_{ij} = \\frac{p_{ij}}{\\sum_{j} p_{ij}}\\,.\n",
    "\\end{align*}\n",
    "For the HMM we need to compute two marginal probabilities:\n",
    "* $\\gamma_{j}(i)$ – probability that the $i$th internal state is $j$ given the observation vector and HMM parameters.\n",
    "* $\\xi_{jk}(i)$ – probability that the $i$th and $(i+1)$th internal states are $j$ and $k$ given the observation vector and HMM parameters.\n",
    "\n",
    "More formally,\n",
    "\n",
    "\\begin{align*}\n",
    "\\gamma_{j}(i)&=\\Pr[x_i=j|\\boldsymbol{y},\\boldsymbol{\\Theta}]\\,,\\\\\n",
    "\\xi_{jk}(i)&=\\Pr[x_i=j, x_{i+1}=k|\\boldsymbol{y},\\boldsymbol{\\Theta}]\\,.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. M-step formulaes for clustering and HMM\n",
    "\n",
    "For the clustering and HMM emission probabilities, we can use Gaussian mixtures as discussed in the previous exercise session:\n",
    "\n",
    "* [03_concepts_behind_expectation_maximisation_algorithm](../07/03_concepts_behind_expectation_maximisation_algorithm.ipynb)\n",
    "\n",
    "For a single observation sequence, the HMM transition probabilities can be calculated using the following formulae:\n",
    "\n",
    "\\begin{align*}\n",
    "\\beta_j&=\\gamma_{j}(1)\\,,\\\\\n",
    "\\alpha_{jk}&=\\frac{\\sum_{i} \\xi_{jk}(i)}{\\sum_i \\gamma_j(i)}\\,.\n",
    "\\end{align*}\n",
    "\n",
    "The generalisation to multiple observation sequences as in our problem is obvious – we just compute sums over all observations and normalise appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implement the EM-algorithm  for the HMM with normal emissions (<font color='red'>3p</font>)\n",
    "\n",
    "Implement weight computation for the HMM. The computation of $\\gamma_{j}(i)$ has already been done in the HMM exercise session, as this is a marginal state probability given all observations. For the weights $\\xi_{jk}(i)$, you need to define a computation scheme that is analogous. Check the consistency of your derivations through the following formula:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\gamma_{j}(i)=\\sum_k\\xi_{ik}(i)\\,.\n",
    "\\end{align*}\n",
    "\n",
    "Implement the M-step by updating the parameters and then assemble the entire algorithm. Use simple 100 iterations as a stopping criterion. Run the algorithm on the dataset and output model parameters.\n",
    "Redo the visual annotations by showing 4 types of state assignments as in notebook \n",
    "[03_change_detection_with_hidden_markov_models.ipynb](../03/03_change_detection_with_hidden_markov_models.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Experiment with the model structure (<font color='red'>3p</font>)\n",
    "\n",
    "By default the EM-algorithm adjusts all model parameters. Try different models:\n",
    "\n",
    "* model where the transition matrix is fixed and you must learn only the emission distribution\n",
    "* model where the number of states is three and all parameters are free\n",
    "* model with cascading states as described in exercise 3.1 in notebook  [03_change_detection_with_hidden_markov_models.ipynb](../03/03_change_detection_with_hidden_markov_models.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
