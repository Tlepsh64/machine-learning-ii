{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping\n",
    "\n",
    "In the following we explain and analyse properties of bootstrapping methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import sklearn\n",
    "\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "\n",
    "from tqdm import tnrange#, tqdm_notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from plotnine import *\n",
    "\n",
    "# Local imports\n",
    "from common import *\n",
    "from convenience import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Experiment setup\n",
    "\n",
    "We again consider a relatively simple prediction task with a relatively small feature set and an impossible prediction task with the same feature set for comparison. \n",
    "We use majority voting and logistic regression as example classifiers as in the previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_0 = lambda n: data_sampler(n, 8, lambda x: logit(x, Series([0, 0])))\n",
    "sampler_1 = lambda n: data_sampler(n, 8, lambda x: logit(x, Series([1, 1])))\n",
    "clf_1 = MajorityVoting()\n",
    "clf_2 = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Naive bootstrap method\n",
    "\n",
    "The must simplest application of bootstrap principle is to sample $B$ bootstap samples $\\mathcal{Z}_b$, i.e., sample $B$ times from the set $\\{(\\boldsymbol{x}_1, y_1),\\ldots (\\boldsymbol{x}_N, y_N)\\}$ with replacement. For each of these samples $\\mathcal{Z}_b$, we can compute empirical risk estimator on the entire dataset\n",
    "\\begin{align*}\n",
    "E_b=\\frac{1}{B}\\cdot\\sum_{b=1}^B\\frac{1}{N}\\cdot\\sum_{i=1}^N  L(y_i, f_b(\\boldsymbol{x}_i))\n",
    "\\end{align*}\n",
    "where $f_b(\\boldsymbol{x_i})$ is the prediction for $\\boldsymbol{x}_i$ trained on $\\mathcal{Z}_b$.\n",
    "This naive bootstrap estimate on the test error will be to optimistic as we estimate the risk on set that is some weird mixture of training and test samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Leave-one-out bootstrap method\n",
    "\n",
    "It is possible to to modify leave-one-out crossvalidation scheme so that the training set is not $n-1$ elements but rather a bootstrap sample over these $n-1$ elements. Formally this requires us to define $n$ different bootstrap distributions, but we can do simple rejection sampling. We sample $B$ times from the set $\\{(\\boldsymbol{x}_1, y_1),\\ldots (\\boldsymbol{x}_N, y_N)\\}$ with replacement and reject these cases where $(\\boldsymbol{x}_i, y_i)$ is inside the sample. This leads to the following error estimate\n",
    "\\begin{align*}\n",
    "E_b^*=\\frac{1}{N}\\cdot\\sum_{i=1}^N \\frac{1}{|C_i|}\\cdot \\sum_{b\\in C_i} L(y_i, f_b(\\boldsymbol{x}_i))\n",
    "\\end{align*}\n",
    "where $C_i$ is the set of indices b such that the bootstrap sample $\\mathcal{Z}_b$ does not contain $(\\boldsymbol{x}_i, y_i)$ and $f_b(\\boldsymbol{x_i})$ is the corresponding prediction for $\\boldsymbol{x}_i$ trained on $\\mathcal{Z}_b$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. .632 bootstrap method\n",
    "\n",
    "The leave-one-out bootstrap estimate $E_b^*$ is too concervative, as the predictor is trained on the training set that on the average consists of $0.632$ different datapoints. \n",
    "The training error\n",
    "\\begin{align*}\n",
    "E_t = \\frac{1}{N}\\cdot\\sum_{i=1}^N L(y_i, f(\\boldsymbol{x}_i))\\enspace,\n",
    "\\end{align*}\n",
    "where $f_b(\\boldsymbol{x}_i)$ is a predictor for $\\boldsymbol{x}_i$ trained on the entire dataset, is too optimistic estimate. \n",
    "The average \n",
    "\n",
    "\\begin{align*}\n",
    "E_{.632}= 0.632\\cdot E_b^* + 0.368\\cdot E_t \n",
    "\\end{align*}\n",
    "\n",
    "is a reasonable tradeoff between both estimates. This is called .632 bootstrap method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. .632+ bootstrap method\n",
    "\n",
    "The .632 bootstrap method works rather nicely, except for the cases where the training error $E_t$ is zero or close to zero due to overfitting. \n",
    "To counter this issue we can define a correction by estimating the relative overfitting rate\n",
    "\\begin{align*}\n",
    "R=\\frac{E_b^*-E_t}{\\gamma- E_t}\n",
    "\\end{align*}\n",
    "where \n",
    "\\begin{align*}\n",
    "\\gamma = \\frac{1}{N^2}\\cdot\\sum_{i=1}^N \\sum_{j=1}^N L(y_i, f(\\boldsymbol{x}_j))\\enspace,\n",
    "\\end{align*}\n",
    "is the expected loss when we predict $y_i$ based on randomly chosen $\\vec{x}_j$ value using the predictor $f$ trained on the entire dataset.\n",
    "\n",
    "From the overfitting rate we can compute modified weights for combining the errors\n",
    "\\begin{align*}\n",
    "E_{.362+}=w\\cdot E_b^*+ (1-w)\\cdot E_t\n",
    "\\end{align*}\n",
    "where \n",
    "\\begin{align*}\n",
    "w=\\frac{0.632}{1-0.632\\cdot R}\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Note that when there is no overfitting $E_b^*=E_t$, then the .632+ bootstrap estimate is the same as .632 estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Comparison of bootstrap methods (<font color='red'>2p</font>)\n",
    "\n",
    "Implement basic bootstrapping algorithm that draws $n$ samples randomly with replacement from $n$-element data set.\n",
    "You can use `DataFrame.samlpe(n, replace=True)` for that. On top of that implement all bootstrap estimates and compare their behaviour on four example cases:\n",
    "* For each data source and algorithm pair draw around 1000 datasets of size 100\n",
    "* For each of these datasets compute compute $E_b, E_b^*, E_t, E_{.632}, E_{.632+}$\n",
    "* For each of these datasets also sample $n$-element independent testset and compute holdout error $E_h$\n",
    "* Visualise results by drawing violin and boxplots\n",
    "* Interpret results. Which od those estimates is closest to $E_h$? Why some estimates are biased?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Analysis of prediction stability (<font color='red'>3p</font>)\n",
    "\n",
    "Bootstrapping can be used to study the stability of a learning algorithm:\n",
    "\n",
    "* You can study how much do coefficients of you model vary.\n",
    "* You can study how fragile is your learning algorithm to noise.\n",
    "\n",
    "Lets explore these concepts by studing the stability of polynomial regression models $y\\sim x^2+x+1$ and $y\\sim x^8 + x^7 + \\cdots + x + 1$.\n",
    "\n",
    "* Stability of coefficients (<font color='red'>1p</font>) \n",
    "  * Fit these models on bootstrapped data and observe regression coefficients by drawing corresponding boxplots. \n",
    "  * Study the mean and variance of individual model coefficients. Declare that a coefficient is insignificant and set it to zero when its mean is not more than 3 standard deviations away from the zero. \n",
    "  * Interpret the results. Are both models similar?\n",
    "  \n",
    "  \n",
    "* Stability of predictions (<font color='red'>1p</font>) \n",
    "  * Fit these models on bootstrapped data.\n",
    "  * For each learned model compute prediction line in the interval $[-2,1]$.\n",
    "  * Draw a faceted plot with facets for models $y\\sim x^2+x+1$ and $y\\sim x^8 + x^7 + \\cdots + x + 1$.\n",
    "  * On each subplot plot individual prediction lines. Use `alpha=0.5` to make lines semi-transparent.\n",
    "  * Draw also average prediction line in red on the plot.\n",
    "\n",
    "\n",
    "* Stability against noise (<font color='red'>1p</font>) \n",
    "  * To study robustness against noise, you can add additional Gaussian noise to $y_i$ values of bootstrapped samples     and later estimate how much did mean square error increase as a consequence. \n",
    "  * The latter should estimate how sensitive is your method to random noise.\n",
    "  * Experiment with different scale values $\\sigma=0.001, 0.01, 0.1, 1$ and visualise results.\n",
    " \n",
    "  \n",
    "\n",
    "### Remarks\n",
    "* Use the following sampler `regr_sampler` as the data source. \n",
    "* Use [sklearn.linear_model.LinearRegression](\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) together with \n",
    "[sklearn.preprocessing.PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) to implement polynomial regression:\n",
    "  * First define additional columns $x_2=x^2, \\ldots, x_8=x^8$.\n",
    "  * Then use linear regression for find corresponding coefficients $\\beta_0,\\beta_1,\\ldots, \\beta_8$.\n",
    "* Use [numpy.random.normal](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html) to sample the additional Gaussian noise needed in the last part of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regr_sampler(n: int) -> DataFrame:\n",
    "    return (DataFrame(np.random.uniform(low=-2, high=1, size=100), columns=['x']).\n",
    "            assign(y = lambda df: df['x']**2 + df['x'] + np.random.normal(scale=0.3, size=len(df))))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c19bf8f28>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "regr_sampler(100).plot(x = 'x', y = 'y', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Bootstrap and its reduction to Monte Carlo crossvalidation* (<font color='red'>3p</font>)\n",
    "\n",
    "Note that the same datapoint can occur many times in a bootstrap sample. \n",
    "This does not play nicely with methods that ignore the sample count in training such as Support Vector Machines that fix the decision border based on the support vectors alone. Study this phenomenon by considering simple linear classification task. Compute leave-one-out bootstrap estimate $E_b^*$ and compare it with error estimate $E_{mc}$ of Monte Carlo crossvalidation scheme with ratios $0.632:0.368$ between training and test set.\n",
    "\n",
    "* Do both methods obatain the same average error estimate? \n",
    "* If not which of them is closer to the true test error  for the model trained over the entire dataset?\n",
    "* Estimate the variances of $E_b^*$ and $E_{mc}$. Are they comparable?\n",
    "* Is there a difference if you consider a hard SVM that ignores multiplicity and soft SVM that considers multiplicity through hindge loss? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
