{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import numpy.random as rnd\n",
    "import string\n",
    "\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "from typing import List\n",
    "\n",
    "from tqdm import tnrange\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Markov chain as language model \n",
    "There are many methods for detecting language of a text document. \n",
    "The simplest one is based on Markov cains of order one, i.e., the last letter determines the probabilities for the next letter.\n",
    "It is terribly naive but it useful for language detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Likelihood and data generation\n",
    "\n",
    "Let $\\beta[x]$ denote the probability that the first letter is $x$. Let $\\alpha[x,y]$ denote the probability that the next letter is $y$ provided that the last letter was $x$.\n",
    "Then we can easily estimate that a word $x_0,\\ldots, x_n$ came from this distribution\n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr[\\boldsymbol{x}|\\alpha,\\beta]= \\beta[x_0]\\cdot\\prod_{i=1}^n \\alpha[x_{i-1},x_i]\n",
    "\\end{align*}\n",
    "\n",
    "Let us define likelihood for a truly random lower-case language as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Naive parameter estimation\n",
    "\n",
    "The parameters of the Markov Chain can be computed by looking at relative frequency of start symbols and bigrams in words. The naive maximum likelihood estimates are \n",
    "\n",
    "\\begin{align*}\n",
    " \\beta[x]&=\\frac{\\# \\text{words starting with }x}{\\# \\text{words}}\\\\\n",
    " \\alpha[x,y]&=\\frac{\\# \\text{bigrams of }xy}{\\# \\text{bigrams starting with }x}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Language detection without Laplace smoothing (<font color='red'>1p</font>)\n",
    "\n",
    "Use files `est_training_set.csv` and `eng_training_set.csv` in the directory `data` to learn model parameters $\\alpha$ and $\\beta$ for both languages using maximum likelihood estimates.\n",
    "Put these parameters into the formal model to compute probabilities\n",
    "\n",
    "\\begin{align*}\n",
    "      p_1 &=\\Pr[word|\\mathsf{Estonian}]\\\\\n",
    "      p_2 &=\\Pr[word|\\mathsf{English}]\n",
    "\\end{align*}\n",
    "\n",
    "and then use Bayes formula\n",
    "\n",
    "\\begin{align*}\n",
    " \\Pr[\\mathsf{Estonian}|word]\n",
    " =\\frac{\\Pr[word|\\mathsf{Estonian}]\\Pr[\\mathsf{Estonian}]}{\\Pr[word]}\n",
    "\\end{align*}\n",
    "to guess the language of a word on test samples `est_test_set.csv` and `eng_test_set.csv`.\n",
    "Why the procedure does not work? \n",
    "\n",
    "**Hint:** The number of samples is not the problem. You can assume that there are enough samples to estimate all parameters with high accuracy. The same problem could have manifested even if there would have been millions of word examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "**We created a language class for estimating the parameters $\\alpha$ and $\\beta$ for each language, and to calculate the probability. In order to be able to estimate whether an Estonian word is English (and vice versa), we used as alphabet the union of the alphabets of both languages (all possible letters seen in the training data).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class language:\n",
    "    def __init__(self, path):\n",
    "        self.letters = set()\n",
    "        self.first_letters = defaultdict(int)\n",
    "        self.letter_pairs = defaultdict(int)\n",
    "        self.count_letters(path)\n",
    "        \n",
    "        self.beta = Series()\n",
    "        self.alpha = DataFrame()\n",
    "        \n",
    "    def count_letters(self, path):\n",
    "        with open(path, 'r') as infile:\n",
    "            for line in infile:\n",
    "                word = line.strip().lower()\n",
    "                if word.startswith('\"') and word.endswith('\"'):\n",
    "                    word = word[1:-1]\n",
    "                if len(word) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                self.first_letters[word[0]] += 1\n",
    "                self.letters.add(word[0])        \n",
    "                for pair in zip(word, word[1:]):\n",
    "                    self.letters.add(pair[1])\n",
    "                    self.letter_pairs[pair] += 1\n",
    "                    \n",
    "    def estimate_parameters(self, alphabet, laplace):\n",
    "        self.estimate_alpha(alphabet, laplace)\n",
    "        self.estimate_beta(alphabet, laplace)\n",
    "    \n",
    "    def estimate_alpha(self, alphabet, laplace):\n",
    "        self.alpha = DataFrame(np.zeros((len(alphabet), len(alphabet))) + laplace, index = alphabet, columns = alphabet)\n",
    "        \n",
    "        for pair, count in self.letter_pairs.items():\n",
    "            self.alpha.loc[pair[0], pair[1]] += count\n",
    "        \n",
    "        self.alpha = self.alpha.apply(lambda x: x if np.sum(x) == 0 else x / np.sum(x), axis = 1)\n",
    "        \n",
    "    def estimate_beta(self, alphabet, laplace):\n",
    "        self.beta = Series(np.zeros(len(alphabet)) + laplace, index = alphabet)\n",
    "        \n",
    "        n = np.sum([count for count in self.first_letters.values()])\n",
    "        for letter, count in self.first_letters.items():\n",
    "            self.beta[letter] += count\n",
    "            \n",
    "        self.beta = self.beta / self.beta.sum()\n",
    "            \n",
    "    def word_probability(self, word):\n",
    "        if len(word) == 0:\n",
    "            return 0\n",
    "        \n",
    "        prob = self.beta[word[0]]\n",
    "        for first, second in zip(word, word[1:]):\n",
    "            prob *= self.alpha.loc[first, second]\n",
    "            \n",
    "        return prob\n",
    "    \n",
    "estonian = language('data/est_training_set.csv')\n",
    "english = language('data/eng_training_set.csv')\n",
    "alphabet = estonian.letters.union(english.letters)\n",
    "\n",
    "estonian.estimate_parameters(alphabet, laplace = 0)\n",
    "english.estimate_parameters(alphabet, laplace = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Having trained the language models, we used these to estimate words on the test data of both languages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy classifying Estonian words: 0.9\n",
      "Accuracy classifying English words: 0.53\n",
      "Total accuracy classifying words: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/machine-learning/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "def estimate_language(path, language_for, language_against, prior = np.array([0.5, 0.5])):\n",
    "    estimates = []\n",
    "    \n",
    "    with open(path, 'r') as infile:\n",
    "        for line in infile:\n",
    "            word = line.strip().lower()\n",
    "            if word.startswith('\"') and word.endswith('\"'):\n",
    "                word = word[1:-1]\n",
    "            if len(word) == 0:\n",
    "                continue\n",
    "            \n",
    "            likelihood_for = language_for.word_probability(word)\n",
    "            likelihood_against = language_against.word_probability(word)\n",
    "            \n",
    "            posterior = np.array([likelihood_for, likelihood_against]) * prior\n",
    "            posterior = posterior / sum(posterior)\n",
    "            \n",
    "            estimates.append(posterior[0] > posterior[1])\n",
    "            \n",
    "    return estimates\n",
    "\n",
    "estonian_estimates = estimate_language('data/est_test_set.csv', estonian, english)\n",
    "english_estimates = estimate_language('data/eng_test_set.csv', english, estonian)\n",
    "print('Accuracy classifying Estonian words: {}'.format(np.mean(estonian_estimates)))\n",
    "print('Accuracy classifying English words: {}'.format(np.mean(english_estimates)))\n",
    "print('Total accuracy classifying words: {}'.format(np.mean([estonian_estimates] + [english_estimates])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The procedure fails for some words if their likelihood in both languages is $0$. In that case the normalizing term of the posterior distribution also becomes $0$ and we have a $0/0$ division. Of course, to eliminate the warning, we could have just used the likelihoods to determine the language of our words. However, that would not have helped against the rather poor performance of our classifier as we would still not have known where to classify a word with zero likelihoods for both languages.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Language detection with Laplace smoothing (<font color='red'>1p</font>)\n",
    "\n",
    "Use files `est_training_set.csv` and `eng_training_set.csv` in the directory `data` to learn model parameters $\\alpha$ and $\\beta$ for both languages using Laplace smoothing.\n",
    "Put these parameters into the formal model to compute probabilities\n",
    "\n",
    "\\begin{align*}\n",
    "      p_1 &=\\Pr[word|\\mathsf{Estonian}]\\\\\n",
    "      p_2 &=\\Pr[word|\\mathsf{English}]\n",
    "\\end{align*}\n",
    "\n",
    "and then use Bayes formula\n",
    "\n",
    "\\begin{align*}\n",
    " \\Pr[\\mathsf{Estonian}|word]\n",
    " =\\frac{\\Pr[word|\\mathsf{Estonian}]\\Pr[\\mathsf{Estonian}]}{\\Pr[word]}\n",
    "\\end{align*}\n",
    "to guess the language of a word on test samples `est_test_set.csv` and `eng_test_set.csv`.\n",
    "Did the problem dissapear? If not then what we have to consider if we apply Laplace smoothing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To overcome the problem outlined above, we retrained our language models using Laplace smoothing, i.e. added a small value to every possible starting letter and bigram before normalization to $\\alpha$ and $\\beta$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy classifying Estonian words: 0.96\n",
      "Accuracy classifying English words: 0.79\n",
      "Total accuracy classifying words: 0.875\n"
     ]
    }
   ],
   "source": [
    "estonian.estimate_parameters(alphabet, laplace = 1)\n",
    "english.estimate_parameters(alphabet, laplace = 1)\n",
    "\n",
    "estonian_estimates = estimate_language('data/est_test_set.csv', estonian, english)\n",
    "english_estimates = estimate_language('data/eng_test_set.csv', english, estonian)\n",
    "print('Accuracy classifying Estonian words: {}'.format(np.mean(estonian_estimates)))\n",
    "print('Accuracy classifying English words: {}'.format(np.mean(english_estimates)))\n",
    "print('Total accuracy classifying words: {}'.format(np.mean([estonian_estimates] + [english_estimates])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see quite a big improvement in the classification, especially on English words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
